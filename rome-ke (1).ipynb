{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:22:37.543220Z","iopub.execute_input":"2026-02-18T12:22:37.543868Z","iopub.status.idle":"2026-02-18T12:22:37.952670Z","shell.execute_reply.started":"2026-02-18T12:22:37.543839Z","shell.execute_reply":"2026-02-18T12:22:37.951920Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38db64e91a6425491f1df2a67a71b60"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!pip install -q transformers accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T06:12:30.878551Z","iopub.execute_input":"2026-02-18T06:12:30.879911Z","iopub.status.idle":"2026-02-18T06:12:36.920396Z","shell.execute_reply.started":"2026-02-18T06:12:30.879877Z","shell.execute_reply":"2026-02-18T06:12:36.919143Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"meta-llama/Llama-3.2-1B\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_auth_token=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    use_auth_token=True\n)\n\nmodel.eval()\nprint(\"Model loaded.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:38:25.768742Z","iopub.execute_input":"2026-02-18T12:38:25.769024Z","iopub.status.idle":"2026-02-18T12:38:27.529716Z","shell.execute_reply.started":"2026-02-18T12:38:25.768999Z","shell.execute_reply":"2026-02-18T12:38:27.529221Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1025: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model loaded.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# Downloading Directory","metadata":{}},{"cell_type":"code","source":"!wget https://rome.baulab.info/data/dsets/counterfact.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:24:16.198500Z","iopub.execute_input":"2026-02-18T12:24:16.199208Z","iopub.status.idle":"2026-02-18T12:24:16.967858Z","shell.execute_reply.started":"2026-02-18T12:24:16.199153Z","shell.execute_reply":"2026-02-18T12:24:16.966921Z"}},"outputs":[{"name":"stdout","text":"--2026-02-18 12:24:16--  https://rome.baulab.info/data/dsets/counterfact.json\nResolving rome.baulab.info (rome.baulab.info)... 35.232.255.106\nConnecting to rome.baulab.info (rome.baulab.info)|35.232.255.106|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 45108470 (43M) [application/json]\nSaving to: ‘counterfact.json’\n\ncounterfact.json    100%[===================>]  43.02M   105MB/s    in 0.4s    \n\n2026-02-18 12:24:16 (105 MB/s) - ‘counterfact.json’ saved [45108470/45108470]\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import json\n\nwith open(\"counterfact.json\", \"r\") as f:\n    counterfact = json.load(f)\n\nprint(\"Total examples:\", len(counterfact))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:24:18.333282Z","iopub.execute_input":"2026-02-18T12:24:18.333613Z","iopub.status.idle":"2026-02-18T12:24:18.696710Z","shell.execute_reply.started":"2026-02-18T12:24:18.333582Z","shell.execute_reply":"2026-02-18T12:24:18.695896Z"}},"outputs":[{"name":"stdout","text":"Total examples: 21919\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"counterfact[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:24:21.008554Z","iopub.execute_input":"2026-02-18T12:24:21.008863Z","iopub.status.idle":"2026-02-18T12:24:21.015087Z","shell.execute_reply.started":"2026-02-18T12:24:21.008835Z","shell.execute_reply":"2026-02-18T12:24:21.014270Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'case_id': 0,\n 'pararel_idx': 2796,\n 'requested_rewrite': {'prompt': 'The mother tongue of {} is',\n  'relation_id': 'P103',\n  'target_new': {'str': 'English', 'id': 'Q1860'},\n  'target_true': {'str': 'French', 'id': 'Q150'},\n  'subject': 'Danielle Darrieux'},\n 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native',\n  'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'],\n 'neighborhood_prompts': ['The mother tongue of Léon Blum is',\n  'The native language of Montesquieu is',\n  'François Bayrou, a native',\n  'The native language of Raymond Barre is',\n  'Michel Rocard is a native speaker of',\n  'Jacques Chaban-Delmas is a native speaker of',\n  'The native language of François Bayrou is',\n  'Maurice Genevoix, speaker of',\n  'The mother tongue of François Bayrou is',\n  'Melchior de Vogüé, speaker of'],\n 'attribute_prompts': ['J.\\xa0R.\\xa0R. Tolkien is a native speaker of',\n  'The mother tongue of Douglas Adams is',\n  'The mother tongue of Paul McCartney is',\n  'Elvis Presley is a native speaker of',\n  'Barack Obama, speaker of',\n  'Douglas Adams, speaker of',\n  'Meryl Streep, a native',\n  'George Orwell spoke the language',\n  'George Washington, a native',\n  'Michael Jackson, a native'],\n 'generation_prompts': [\"Danielle Darrieux's mother tongue is\",\n  'Where Danielle Darrieux is from, people speak the language of',\n  \"Danielle Darrieux's mother tongue is\",\n  'Danielle Darrieux was born in',\n  \"Danielle Darrieux's mother tongue is\",\n  \"Danielle Darrieux's mother tongue is\",\n  'Danielle Darrieux was born in',\n  'Where Danielle Darrieux is from, people speak the language of',\n  'Danielle Darrieux was born in',\n  'Danielle Darrieux was born in']}"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"example = counterfact[0]\n\nrw = example[\"requested_rewrite\"]\n\nsubject = rw[\"subject\"]\nprompt = rw[\"prompt\"].format(subject)\n\ntrue_target = rw[\"target_true\"][\"str\"]\nnew_target = rw[\"target_new\"][\"str\"]\n\nparaphrase_prompts = example[\"paraphrase_prompts\"]\nneighborhood_prompts = example[\"neighborhood_prompts\"]\nattribute_prompts = example[\"attribute_prompts\"]\ngeneration_prompts = example[\"generation_prompts\"]\n\nprint(prompt)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:24:24.098403Z","iopub.execute_input":"2026-02-18T12:24:24.098972Z","iopub.status.idle":"2026-02-18T12:24:24.103927Z","shell.execute_reply.started":"2026-02-18T12:24:24.098940Z","shell.execute_reply":"2026-02-18T12:24:24.103213Z"}},"outputs":[{"name":"stdout","text":"The mother tongue of Danielle Darrieux is\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\ninput_ids = inputs[\"input_ids\"][0]\n\nprint(input_ids)\n\ntokens = tokenizer.convert_ids_to_tokens(input_ids)\nfor i, tok in enumerate(tokens):\n    print(f\"{i}: {tok}\")\nprint()\n\ndecoded_tokens = [\n    tokenizer.decode([tok_id])\n    for tok_id in input_ids\n]\nprint(decoded_tokens)\n\n# Reconstruct cumulative string and find subject span for k value\nreconstructed = \"\"\nsubject_index = None\n\nfor i, tok in enumerate(decoded_tokens):\n    reconstructed += tok\n    if subject in reconstructed:\n        subject_index = i\n        break\n\nif subject_index is None:\n    raise ValueError(\"Subject not found in prompt!\")\n\nprint(\"Subject index:\", subject_index) # capturing the index till where the whole subject in included in the sentence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:24:26.248589Z","iopub.execute_input":"2026-02-18T12:24:26.248891Z","iopub.status.idle":"2026-02-18T12:24:26.282513Z","shell.execute_reply.started":"2026-02-18T12:24:26.248865Z","shell.execute_reply":"2026-02-18T12:24:26.281897Z"}},"outputs":[{"name":"stdout","text":"tensor([128000,    791,   6691,  25466,    315,  72716,  15367,   7379,   2249,\n           374], device='cuda:0')\n0: <|begin_of_text|>\n1: The\n2: Ġmother\n3: Ġtongue\n4: Ġof\n5: ĠDanielle\n6: ĠDar\n7: rie\n8: ux\n9: Ġis\n\n['<|begin_of_text|>', 'The', ' mother', ' tongue', ' of', ' Danielle', ' Dar', 'rie', 'ux', ' is']\nSubject index: 8\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**Function to get the log probabilities of the target token**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef get_sequence_logprob(prompt, target):\n    full = prompt + \" \" + target\n    inputs = tokenizer(full, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    log_probs = F.log_softmax(logits[0], dim=-1)\n\n    target_ids = tokenizer(target)[\"input_ids\"][1:]\n    input_ids = inputs[\"input_ids\"][0]\n\n    total = 0.0\n    start = len(input_ids) - len(target_ids)\n\n    for i in range(len(target_ids)):\n        total += log_probs[start + i - 1, target_ids[i]]\n\n    return total.item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:24:29.260703Z","iopub.execute_input":"2026-02-18T12:24:29.260995Z","iopub.status.idle":"2026-02-18T12:24:29.266788Z","shell.execute_reply.started":"2026-02-18T12:24:29.260968Z","shell.execute_reply":"2026-02-18T12:24:29.266196Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(\"=== BEFORE EDIT ===\")\nprint(\"Rewrite true:\", get_sequence_logprob(prompt, true_target))\nprint(\"Rewrite new :\", get_sequence_logprob(prompt, new_target))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:24:29.438219Z","iopub.execute_input":"2026-02-18T12:24:29.438866Z","iopub.status.idle":"2026-02-18T12:24:30.431728Z","shell.execute_reply.started":"2026-02-18T12:24:29.438838Z","shell.execute_reply":"2026-02-18T12:24:30.431042Z"}},"outputs":[{"name":"stdout","text":"=== BEFORE EDIT ===\nRewrite true: -9.8125\nRewrite new : -12.25\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# APPLYING THE ROME EDIT","metadata":{}},{"cell_type":"code","source":"layer_id = 13\nlayer = model.model.layers[layer_id]\n\nactivation = {}\n\ndef hook_fn(module, input, output):\n    activation[\"k\"] = input[0].detach()\n\nhandle = layer.mlp.down_proj.register_forward_hook(hook_fn) # to get the activations from the layer when the layer is run\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    model(**inputs)\n\nhandle.remove()\nprint(activation) # number vectors that represents the sentence/subject here it represents the Subject 'Danielle Darrieux'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:37.940201Z","iopub.execute_input":"2026-02-18T12:25:37.940520Z","iopub.status.idle":"2026-02-18T12:25:38.465242Z","shell.execute_reply.started":"2026-02-18T12:25:37.940492Z","shell.execute_reply":"2026-02-18T12:25:38.464494Z"}},"outputs":[{"name":"stdout","text":"{'k': tensor([[[ 6.0730e-03, -9.3079e-04, -3.5645e-02,  ...,  5.8594e-02,\n           1.2131e-03,  2.2217e-02],\n         [-1.5991e-02,  2.5146e-02, -1.8799e-02,  ..., -1.5259e-02,\n           2.9922e-05,  1.8433e-02],\n         [ 8.7402e-02, -2.0264e-02,  1.1816e-01,  ..., -1.8188e-02,\n           4.0771e-02,  3.4912e-02],\n         ...,\n         [ 5.4199e-02,  1.3184e-01,  3.5645e-02,  ..., -6.7383e-02,\n           8.5938e-02, -1.5442e-02],\n         [-2.2949e-02, -2.9144e-03,  7.7209e-03,  ..., -1.4404e-02,\n           1.2268e-02,  2.9907e-02],\n         [-1.2085e-02, -2.7222e-02, -6.8359e-02,  ..., -1.8311e-02,\n           4.8218e-03, -2.6245e-02]]], device='cuda:1', dtype=torch.bfloat16)}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"input_ids = inputs[\"input_ids\"][0]\ndecoded = [tokenizer.decode([tok]) for tok in input_ids]\n\nreconstructed = \"\"\nsubject_index = None\n\nfor i, tok in enumerate(decoded):\n    reconstructed += tok\n    if subject in reconstructed:\n        subject_index = i\n        break\n\nprint(\"Subject index:\", subject_index)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:38.466464Z","iopub.execute_input":"2026-02-18T12:25:38.466691Z","iopub.status.idle":"2026-02-18T12:25:38.484149Z","shell.execute_reply.started":"2026-02-18T12:25:38.466669Z","shell.execute_reply":"2026-02-18T12:25:38.483620Z"}},"outputs":[{"name":"stdout","text":"Subject index: 8\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"k = activation[\"k\"][0, subject_index].float()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:38.484911Z","iopub.execute_input":"2026-02-18T12:25:38.485243Z","iopub.status.idle":"2026-02-18T12:25:38.488551Z","shell.execute_reply.started":"2026-02-18T12:25:38.485209Z","shell.execute_reply":"2026-02-18T12:25:38.487864Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"print(k.shape) # k -> internal MLP representation (vector embedddings) of Subject token at layer = layer_index\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:38.490286Z","iopub.execute_input":"2026-02-18T12:25:38.490773Z","iopub.status.idle":"2026-02-18T12:25:38.500894Z","shell.execute_reply.started":"2026-02-18T12:25:38.490749Z","shell.execute_reply":"2026-02-18T12:25:38.500124Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8192])\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"target_ids = tokenizer(new_target, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    v = model.model.embed_tokens(target_ids[\"input_ids\"]).mean(dim=1)[0].float()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:38.644398Z","iopub.execute_input":"2026-02-18T12:25:38.645021Z","iopub.status.idle":"2026-02-18T12:25:38.649472Z","shell.execute_reply.started":"2026-02-18T12:25:38.644987Z","shell.execute_reply":"2026-02-18T12:25:38.648859Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"layer_device = layer.mlp.down_proj.weight.device\n\n# Move tensors explicitly\nk = k.to(layer_device)\nv = v.to(layer_device)\n\nW = layer.mlp.down_proj.weight.data\ndevice = W.device\n\n# Convert everything to float32 for more precise calculations\nW_float = W.float()\nk_float = k.to(device).float()\nv_float = v.to(device).float()\n\n# Ensure correct shapes\nassert k_float.dim() == 1\nassert v_float.dim() == 1\n\n# Compute Wk\nWk = W_float @ k_float        # (2048,), vector embeddding for the target \n\n# Compute correction vector\ncorrection = v_float - Wk     # (2048,) v_float is vector for new_target and Wk for true_target \n\n# Outer product\ndelta_W = torch.ger(correction, k_float)  # (2048, 8192) a little change in weights for the subject token \n\n# Normalize\ndelta_W /= (k_float @ k_float) # normalising\n\n# Apply\nlayer.mlp.down_proj.weight.data += delta_W.to(W.dtype)\n\nprint(\"ROME edit applied.\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:41.908703Z","iopub.execute_input":"2026-02-18T12:25:41.909025Z","iopub.status.idle":"2026-02-18T12:25:41.916573Z","shell.execute_reply.started":"2026-02-18T12:25:41.908995Z","shell.execute_reply":"2026-02-18T12:25:41.915954Z"}},"outputs":[{"name":"stdout","text":"ROME edit applied.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"print(\"=== REWRITE CHECK ===\")\nprint(\"True:\", get_sequence_logprob(prompt, true_target))\nprint(\"New :\", get_sequence_logprob(prompt, new_target))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:42.063012Z","iopub.execute_input":"2026-02-18T12:25:42.063467Z","iopub.status.idle":"2026-02-18T12:25:42.397536Z","shell.execute_reply.started":"2026-02-18T12:25:42.063441Z","shell.execute_reply":"2026-02-18T12:25:42.396916Z"}},"outputs":[{"name":"stdout","text":"=== REWRITE CHECK ===\nTrue: -13.5\nNew : -11.1875\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"print(\"\\n=== PARAPHRASE CHECK ===\")\n\nfor p in paraphrase_prompts:\n    print(p)\n    print(\"New:\", get_sequence_logprob(p, new_target))\n    print(\"True:\", get_sequence_logprob(p, true_target))\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:42.398849Z","iopub.execute_input":"2026-02-18T12:25:42.399101Z","iopub.status.idle":"2026-02-18T12:25:42.923413Z","shell.execute_reply.started":"2026-02-18T12:25:42.399078Z","shell.execute_reply":"2026-02-18T12:25:42.922670Z"}},"outputs":[{"name":"stdout","text":"\n=== PARAPHRASE CHECK ===\nShayna does this and Yossel goes still and dies. Danielle Darrieux, a native\nNew: -13.5625\nTrue: -12.5\n\nAn album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language\nNew: -14.1875\nTrue: -17.0\n\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"print(\"\\n=== NEIGHBORHOOD CHECK ===\")\n\nfor p in neighborhood_prompts[:5]:\n    print(p)\n    # print(_target)\n    print(\"True:\", get_sequence_logprob(p, true_target))\n    print(\"New:\", get_sequence_logprob(p, new_target))\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:25:26.031241Z","iopub.execute_input":"2026-02-18T12:25:26.031529Z","iopub.status.idle":"2026-02-18T12:25:26.875041Z","shell.execute_reply.started":"2026-02-18T12:25:26.031506Z","shell.execute_reply":"2026-02-18T12:25:26.874452Z"}},"outputs":[{"name":"stdout","text":"\n=== NEIGHBORHOOD CHECK ===\nThe mother tongue of Léon Blum is\nTrue: -13.125\nNew: -12.9375\n\nThe native language of Montesquieu is\nTrue: -12.0625\nNew: -12.375\n\nFrançois Bayrou, a native\nTrue: -16.125\nNew: -17.0\n\nThe native language of Raymond Barre is\nTrue: -11.875\nNew: -10.9375\n\nMichel Rocard is a native speaker of\nTrue: -11.375\nNew: -10.6875\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\n\noriginal_state = copy.deepcopy(model.state_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:38:42.733770Z","iopub.execute_input":"2026-02-18T12:38:42.734430Z","iopub.status.idle":"2026-02-18T12:38:42.768854Z","shell.execute_reply.started":"2026-02-18T12:38:42.734399Z","shell.execute_reply":"2026-02-18T12:38:42.768338Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def apply_rome_edit(model, tokenizer, example, layer_id=best_layer):\n\n    rw = example[\"requested_rewrite\"]\n    subject = rw[\"subject\"]\n    prompt = rw[\"prompt\"].format(subject)\n    new_target = rw[\"target_new\"][\"str\"]\n\n    layer = model.model.layers[layer_id]\n    activation = {}\n\n    def hook_fn(module, input, output):\n        activation[\"k\"] = input[0].detach()\n\n    handle = layer.mlp.down_proj.register_forward_hook(hook_fn)\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        model(**inputs)\n\n    handle.remove()\n\n    # ---- find subject index ----\n    input_ids = inputs[\"input_ids\"][0]\n    decoded = [tokenizer.decode([tok]) for tok in input_ids]\n\n    reconstructed = \"\"\n    subject_index = None\n\n    for i, tok in enumerate(decoded):\n        reconstructed += tok\n        if subject in reconstructed:\n            subject_index = i\n            break\n\n    k = activation[\"k\"][0, subject_index]\n\n    # ---- compute v ----\n    target_ids = tokenizer(new_target, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        v = model.model.embed_tokens(target_ids[\"input_ids\"]).mean(dim=1)[0]\n\n    # ---- apply rank-one update ----\n    W = layer.mlp.down_proj.weight.data\n    device = W.device\n\n    W_float = W.float()\n    k_float = k.to(device).float()\n    v_float = v.to(device).float()\n\n    Wk = W_float @ k_float\n    correction = v_float - Wk\n    delta_W = torch.ger(correction, k_float)\n    delta_W /= (k_float @ k_float)\n\n    alpha = 5.0\n    layer.mlp.down_proj.weight.data += alpha * delta_W.to(W.dtype)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T13:38:27.839864Z","iopub.execute_input":"2026-02-18T13:38:27.840497Z","iopub.status.idle":"2026-02-18T13:38:27.848522Z","shell.execute_reply.started":"2026-02-18T13:38:27.840465Z","shell.execute_reply":"2026-02-18T13:38:27.847974Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"def find_best_layer(model, tokenizer, example, candidate_layers):\n\n    best_layer = None\n    best_margin = -1e9\n\n    original_state = copy.deepcopy(model.state_dict())\n\n    rw = example[\"requested_rewrite\"]\n    subject = rw[\"subject\"]\n    prompt = rw[\"prompt\"].format(subject)\n    true_target = rw[\"target_true\"][\"str\"]\n    new_target = rw[\"target_new\"][\"str\"]\n\n    for layer_id in candidate_layers:\n\n        model.load_state_dict(original_state)\n\n        apply_rome_edit(model, tokenizer, example, layer_id=layer_id)\n\n        true_score = get_sequence_logprob(prompt, true_target)\n        new_score = get_sequence_logprob(prompt, new_target)\n\n        margin = new_score - true_score\n\n        if margin > best_margin:\n            best_margin = margin\n            best_layer = layer_id\n\n    return best_layer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T13:38:27.983382Z","iopub.execute_input":"2026-02-18T13:38:27.983585Z","iopub.status.idle":"2026-02-18T13:38:27.988533Z","shell.execute_reply.started":"2026-02-18T13:38:27.983565Z","shell.execute_reply":"2026-02-18T13:38:27.987990Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"def rewrite_success(model, tokenizer, example):\n\n    rw = example[\"requested_rewrite\"]\n\n    subject = rw[\"subject\"]\n    prompt = rw[\"prompt\"].format(subject)\n    true_target = rw[\"target_true\"][\"str\"]\n    new_target = rw[\"target_new\"][\"str\"]\n\n    true_score = get_sequence_logprob(prompt, true_target)\n    new_score = get_sequence_logprob(prompt, new_target)\n\n    return new_score > true_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T13:38:28.183926Z","iopub.execute_input":"2026-02-18T13:38:28.184348Z","iopub.status.idle":"2026-02-18T13:38:28.188925Z","shell.execute_reply.started":"2026-02-18T13:38:28.184320Z","shell.execute_reply":"2026-02-18T13:38:28.188214Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"def paraphrase_success(model, tokenizer, example):\n\n    rw = example[\"requested_rewrite\"]\n    new_target = rw[\"target_new\"][\"str\"]\n    true_target = rw[\"target_true\"][\"str\"]\n\n    success = 0\n    total = 0\n\n    for p in example[\"paraphrase_prompts\"]:\n        new_score = get_sequence_logprob(p, new_target)\n        true_score = get_sequence_logprob(p, true_target)\n\n        if new_score > true_score:\n            success += 1\n        total += 1\n\n    return success / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T13:38:28.365452Z","iopub.execute_input":"2026-02-18T13:38:28.365755Z","iopub.status.idle":"2026-02-18T13:38:28.370495Z","shell.execute_reply.started":"2026-02-18T13:38:28.365729Z","shell.execute_reply":"2026-02-18T13:38:28.369784Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"def neighborhood_preservation(model, tokenizer, example):\n\n    rw = example[\"requested_rewrite\"]\n    true_target = rw[\"target_true\"][\"str\"]\n\n    success = 0\n    total = 0\n\n    for p in example[\"neighborhood_prompts\"][:5]:\n        new_score = get_sequence_logprob(p, new_target)\n        true_score = get_sequence_logprob(p, true_target)\n\n        # Ideally compare with original known correct answer\n        if new_score > true_score:\n            success += 1  # placeholder unless you compute proper comparison\n        total += 1\n\n    return success / total\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T13:38:28.551205Z","iopub.execute_input":"2026-02-18T13:38:28.551504Z","iopub.status.idle":"2026-02-18T13:38:28.556249Z","shell.execute_reply.started":"2026-02-18T13:38:28.551478Z","shell.execute_reply":"2026-02-18T13:38:28.555650Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"N = 20\ncandidate_layers = list(range(6, 16))\n\nrewrite_scores = []\nparaphrase_scores = []\nneighborhood_scores = []\n\nfor i, example in enumerate(counterfact[:N]):\n\n    print(f\"\\nCase {i}\")\n\n    # Reset model\n    model.load_state_dict(original_state)\n\n    # Find best layer\n    best_layer = find_best_layer(model, tokenizer, example, candidate_layers)\n\n    # Reset again\n    model.load_state_dict(original_state)\n\n    # Apply final edit\n    apply_rome_edit(model, tokenizer, example, layer_id=best_layer)\n\n    # Evaluate\n    rewrite = rewrite_success(model, tokenizer, example)\n    paraphrase = paraphrase_success(model, tokenizer, example)\n    neighborhood = neighborhood_preservation(model, tokenizer, example)\n\n    rewrite_scores.append(rewrite)\n    paraphrase_scores.append(paraphrase)\n    neighborhood_scores.append(neighborhood)\n\n    print(\"Rewrite:\", rewrite)\n    print(\"Paraphrase:\", paraphrase)\n    print(\"Neighborhood:\", neighborhood)\n\nprint(\"\\n===== FINAL RESULTS =====\")\nprint(\"Rewrite accuracy:\", sum(rewrite_scores)/N)\nprint(\"Paraphrase accuracy:\", sum(paraphrase_scores)/N)\nprint(\"Neighborhood preservation:\", sum(neighborhood_scores)/N)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T13:38:29.035544Z","iopub.execute_input":"2026-02-18T13:38:29.036006Z","iopub.status.idle":"2026-02-18T13:39:42.166915Z","shell.execute_reply.started":"2026-02-18T13:38:29.035979Z","shell.execute_reply":"2026-02-18T13:39:42.166210Z"}},"outputs":[{"name":"stdout","text":"\nCase 0\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 1\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 2\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 3\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 0.0\n\nCase 4\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 5\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 6\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 7\nRewrite: True\nParaphrase: 0.5\nNeighborhood: 0.0\n\nCase 8\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 9\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 0.0\n\nCase 10\nRewrite: True\nParaphrase: 0.0\nNeighborhood: 0.0\n\nCase 11\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 12\nRewrite: True\nParaphrase: 0.0\nNeighborhood: 0.0\n\nCase 13\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 14\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 15\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 16\nRewrite: False\nParaphrase: 0.5\nNeighborhood: 1.0\n\nCase 17\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 18\nRewrite: True\nParaphrase: 1.0\nNeighborhood: 1.0\n\nCase 19\nRewrite: False\nParaphrase: 0.0\nNeighborhood: 0.2\n\n===== FINAL RESULTS =====\nRewrite accuracy: 0.9\nParaphrase accuracy: 0.8\nNeighborhood preservation: 0.71\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}